# ai-paper-implementations
Implementation of papers from scratch

**ai-paper-implementations folder structure:**

├── requirements.txt  
├── classic-ml/       : Implementations of classic machine learning algorithms and papers
├── deep-learning/    : Implementations of deep learning algorithms and papers
├── computer-vision/  : Implementations of image processing and computer vision algorithms and papers
├── nlp/	      : Implementations of NLP related algorithms and papers
├── gen-ai/ 	      : Implementations of Generative AI algorithms and papers


## Implementation Checklist

**Classical ML**

- [ ] Induction of Decision Trees
- [ ] A Training Algorithm for Optimal Margin Classifiers
- [ ] Random Forests
- [ ] The Elements of Statistical Learning
- [ ] A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting
- [ ] Latent Dirichlet Allocation
- [ ] Map-Reduce: Simplified Data Processing on Large Clusters
- [ ] A Few Useful Things to Know about Machine Learning
- [ ] XGBoost: A Scalable Tree Boosting System

**Deep Learning**

- [ ] Learning representations by back-propagating errors
- [ ] Long Short-Term Memory
- [ ] Deep Boltzmann Machines
- [ ] ImageNet Classification with Deep Convolutional Neural Networks
- [ ] Dropout: A Simple Way to Prevent Neural Networks from Overfitting
- [ ] Adam: A Method for Stochastic Optimization
- [ ] Deep Residual Learning for Image Recognition
- [ ] Attention Is All You Need
- [ ] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- [ ] A Survey of Deep Learning Techniques for Neural Machine Translation
- [ ] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- [ ] Generative Adversarial Nets
- [ ] High-Resolution Image Synthesis with Latent Diffusion Models
- [ ] Evaluating Large Language Models Trained on Code

**Computer-Vision**

- [ ] Scale-Invariant Feature Transform (SIFT)
- [ ] Rapid Object Detection using a Boosted Cascade of Simple Features
- [ ] Histograms of Oriented Gradients for Human Detection
- [ ] A Performance Evaluation of Local Descriptors
- [ ] Very Deep Convolutional Networks for Large-Scale Image Recognition
- [ ] Rich feature hierarchies for accurate object detection and semantic segmentation
- [ ] U-Net: Convolutional Networks for Biomedical Image Segmentation
- [ ] Mask R-CNN
- [ ] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- [ ] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
- [ ] Learning Transferable Visual Models From Natural Language Supervision
- [ ] High-Resolution Image Synthesis with Latent Diffusion Models
- [ ] Segment Anything

**NLP**

- [ ] A Statistical Approach to Machine Translation
- [ ] GloVe: Global Vectors for Word Representation
- [ ] Sequence to Sequence Learning with Neural Networks
- [ ] Attention Is All You Need
- [ ] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- [ ] Language Models are Unsupervised Multitask Learners
- [ ] RoBERTa: A Robustly Optimized BERT Pretraining Approach
- [ ] ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
- [ ] Language Models are Few-Shot Learners
- [ ] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
- [ ] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
- [ ] InstructGPT: Training language models to follow instructions with human feedback
- [ ] Constitutional AI: Harmlessness from AI Feedback
- [ ] Self-Instruct: Aligning Language Model with Self Generated Instructions

**Gen AI**

- [ ] Generative Adversarial Nets
- [ ] Auto-Encoding Variational Bayes
- [ ] Learning Transferable Visual Models From Natural Language Supervision
- [ ] Zero-Shot Text-to-Image Generation
- [ ] High-Resolution Image Synthesis with Latent Diffusion Models
- [ ] Flamingo: a Visual Language Model for Few-Shot Learning
- [ ] PaLM-E: An Embodied Multimodal Language Model
- [ ] Scaling Autoregressive Models for Content-Rich Text-to-Image Generation
- [ ] Muse: Text-To-Image Generation via Masked Generative Transformers
- [ ] VideoGPT: Video Generation using VQ-VAE and Transformers
- [ ] Make-A-Video: Text-to-Video Generation without Text-Video Data
- [ ] Robust Speech Recognition via Large-Scale Weak Supervision 


